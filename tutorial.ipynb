{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install\n",
    "\n",
    "Python 3 only:\n",
    "```bash\n",
    "pip install canton\n",
    "```\n",
    "\n",
    "## Usage\n",
    "\n",
    "Import the essentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import canton as ct\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_variable = tf.Variable(np.random.normal(loc=0,scale=1,size=[1,256,256,3]\n",
    "    ).astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then feed it through three 2-D convolutional layers, where:\n",
    "- conv_0 has its own weights\n",
    "- conv_1 and conv_2 share weights\n",
    "\n",
    "In order to do this we first create 2 convolutional layers, each with its own set of weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tensorflow.python.ops.variables.Variable object at 0x0000000009F81780>, <tensorflow.python.ops.variables.Variable object at 0x0000000009F95BE0>]\n",
      "[<tensorflow.python.ops.variables.Variable object at 0x0000000009FA4358>, <tensorflow.python.ops.variables.Variable object at 0x0000000009FBA940>]\n"
     ]
    }
   ],
   "source": [
    "conv = ct.Conv2D(3,16,3)\n",
    "shared_conv = ct.Conv2D(16,16,3)\n",
    "print(conv.weights)\n",
    "print(shared_conv.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then simply apply the second layer twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_2:0\", shape=(1, 256, 256, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "i = conv(input_variable)\n",
    "i = shared_conv(i)\n",
    "out = shared_conv(i)\n",
    "print(out)\n",
    "\n",
    "# define loss\n",
    "loss = tf.reduce_mean(out**2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's assume you only want to train the shared layer's weights (keep the first `conv` layer's weight frozen). Instead of using `tf.get_collection(some_keys_you_have_to_remember)`, or `get_layer('some_name').trainable = False`, you simply pick the weights you want to train and throw them into `optimizer.minimize()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "opt = tf.train.AdamOptimizer(1e-3)\n",
    "# define train op\n",
    "train_step = opt.minimize(loss,var_list=shared_conv.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems stupid (doing more work than Keras) at first glance, but super handy if you happen to be training GANs or anything NOT for Kaggle competitions.\n",
    "\n",
    "Now you can train it the TensorFlow way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.80577\n",
      "loss: 3.61594\n",
      "loss: 3.43501\n",
      "loss: 3.26269\n",
      "loss: 3.09886\n",
      "loss: 2.94324\n",
      "loss: 2.79556\n",
      "loss: 2.65554\n",
      "loss: 2.52299\n",
      "loss: 2.39752\n"
     ]
    }
   ],
   "source": [
    "sess = ct.get_session() # just the TF Session\n",
    "sess.run(tf.global_variables_initializer()) # initialize all weights\n",
    "for i in range(10):\n",
    "    res = sess.run([train_step,loss],feed_dict={}) # you should feed inputs if you have\n",
    "    print('loss:',res[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok the loss is decreasing, which means the weights are getting trained. Now let's assume you like this \"2Conv1Weight\" idea very much, and wanna apply this layer two more times to your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = shared_conv(out)\n",
    "out = shared_conv(out)\n",
    "\n",
    "# redefine loss\n",
    "loss = tf.reduce_mean(out**2.)\n",
    "# redefine train op (Note: do not redefine the optimizer, which will produce error due to variable scope clashing)\n",
    "train_step = opt.minimize(loss,var_list=shared_conv.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have to reinitialize all the variables, since the previous session is still open:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.11723\n",
      "loss: 4.72265\n",
      "loss: 4.32541\n",
      "loss: 3.94039\n",
      "loss: 3.57698\n",
      "loss: 3.24042\n",
      "loss: 2.9328\n",
      "loss: 2.65436\n",
      "loss: 2.40401\n",
      "loss: 2.18001\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    res = sess.run([train_step,loss],feed_dict={})\n",
    "    print('loss:',res[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the loss values, the weights are not lost between two runs. Now let's assume you wanna save the weights to a file (in numpy format) for future uses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 weights (and variables) obtained.\n",
      "successfully saved to shared_conv.npy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_conv.save_weights('shared_conv.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you train the model for some more steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.98021\n",
      "loss: 1.80226\n",
      "loss: 1.6439\n",
      "loss: 1.50296\n",
      "loss: 1.37744\n",
      "loss: 1.26553\n",
      "loss: 1.16558\n",
      "loss: 1.07618\n",
      "loss: 0.996039\n",
      "loss: 0.924061\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    res = sess.run([train_step,loss],feed_dict={})\n",
    "    print('loss:',res[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the loss is too low, showing signs of overfitting. Assume you want to revert your weights to the last checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded from shared_conv.npy\n",
      "2 weights assigned.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_conv.load_weights('shared_conv.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model again. As you can see the loss values increased back to our previous checkpoint. (However the training dynamic governed by the Adam optimizer didn't change, so the results are not going to be exactly identical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.98021\n",
      "loss: 1.83788\n",
      "loss: 1.70334\n",
      "loss: 1.57729\n",
      "loss: 1.46005\n",
      "loss: 1.35165\n",
      "loss: 1.2519\n",
      "loss: 1.16046\n",
      "loss: 1.07685\n",
      "loss: 1.00061\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    res = sess.run([train_step,loss],feed_dict={})\n",
    "    print('loss:',res[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept of Cans\n",
    "\n",
    "`Can` (`from canton import Can`) is the base class for the Conv2D layer above.\n",
    "\n",
    "A Can is basically a container of actions and its associated weights.\n",
    "\n",
    "When a Can is initialized, all its weight variables are created (but not initialized of course).\n",
    "\n",
    "Every Can is callable after initialization. By calling a Can on a tensor, for example `i = shared_conv(i)`, you extend the computation graph and obtain a result tensor just like with TensorFlow, however no new weights will be created during the call. The weight is **shared** among all its calls.\n",
    "\n",
    "As seen above, you can very easily save or restore the weights of a Can, or retrieve them as tensors. So, why not represent bigger building blocks, or even the whole network as a Can? That way we could build networks of arbitary complexity, and train them in interesting ways (like adding adversarial loss), without ever having to memorize all those variable names and scopes...\n",
    "\n",
    "Yes, you can create Cans consisting of other Cans: that creates a Can Hierarchy.\n",
    "\n",
    "## Can Hierarchy\n",
    "\n",
    "Assume you came up with a new idea: Create two convolutional layer A and B, apply them one-after-another to the input N times:\n",
    "\n",
    "- `i = B(A(i))` for N=1;\n",
    "\n",
    "- `i = B(A(B(A(i))))` for N=2;\n",
    "\n",
    "so why not combine A and B into one Can, and call that N times over the input? Then we only have to call `get_weights()`once to train with optimizer, call `save_weights()` once to save the parameters.\n",
    "\n",
    "Here's the default class inheritance approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DoubleConv(ct.Can):\n",
    "    def __init__(self):\n",
    "        super().__init__() # init base class\n",
    "        self.convs = [ct.Conv2D(3,16,3),ct.Conv2D(16,3,3)] # define conv2d cans\n",
    "        self.incan(self.convs) # add as subcans\n",
    "    def __call__(self,i):\n",
    "        i = self.convs[0](i)\n",
    "        i = self.convs[1](i)\n",
    "        return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: I know it's verbose. You don't always have to do that. Just keep reading.\n",
    "\n",
    "By calling `self.incan(cans)`, you add one or more Can(s) as the **SubCan(s)** of the Can. You can access the list of a Can's SubCans via its **subcans** property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<canton.cans.Conv2D object at 0x000000000A1DC9B0>, <canton.cans.Conv2D object at 0x000000000A1BA748>]\n"
     ]
    }
   ],
   "source": [
    "dc = DoubleConv()\n",
    "print(dc.subcans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can of course get its weights: It will traverse the hierarchy tree and collect weight tensors from its subcans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tensorflow.python.ops.variables.Variable object at 0x000000000A1DC9E8>, <tensorflow.python.ops.variables.Variable object at 0x000000000A1DCA58>, <tensorflow.python.ops.variables.Variable object at 0x000000000A1BA898>, <tensorflow.python.ops.variables.Variable object at 0x000000000A1DD6A0>]\n"
     ]
    }
   ],
   "source": [
    "print(dc.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's right, 2 convolutions needs 4 variables (2 weights and 2 biases). \n",
    "\n",
    "And yes, you can call it and train it, just like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 7.93796\n",
      "loss: 7.23227\n",
      "loss: 6.59103\n",
      "loss: 6.00942\n",
      "loss: 5.48248\n",
      "loss: 5.00526\n",
      "loss: 4.57313\n",
      "loss: 4.18187\n",
      "loss: 3.82753\n",
      "loss: 3.50663\n"
     ]
    }
   ],
   "source": [
    "i = dc(input_variable)\n",
    "out = dc(i) # N=2\n",
    "\n",
    "loss = tf.reduce_mean(out**2.)\n",
    "train_step = opt.minimize(loss, var_list=dc.get_weights())\n",
    "\n",
    "sess.run(tf.global_variables_initializer()) # init and re-init all the weights (mainly for the optimizer)\n",
    "for i in range(10):\n",
    "    res = sess.run([train_step,loss],feed_dict={})\n",
    "    print('loss:',res[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we can save and restore the Can:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 weights (and variables) obtained.\n",
      "successfully saved to test.npy\n",
      "successfully loaded from test.npy\n",
      "4 weights assigned.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc.save_weights('test.npy')\n",
    "dc.load_weights('test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Facts\n",
    "\n",
    "Class inheritance is boring. Is there any better ways to assemble a Can? Well you may use closure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.59758\n",
      "loss: 2.4894\n",
      "loss: 2.38474\n",
      "loss: 2.28357\n",
      "loss: 2.18589\n",
      "loss: 2.09167\n",
      "loss: 2.00088\n",
      "loss: 1.9135\n",
      "loss: 1.82947\n",
      "loss: 1.74873\n"
     ]
    }
   ],
   "source": [
    "def DoubleConv2():\n",
    "    can = ct.Can()\n",
    "    convs = [ct.Conv2D(3,16,3),ct.Conv2D(16,3,3)]\n",
    "    def call(i):\n",
    "        i = convs[0](i)\n",
    "        i = convs[1](i)\n",
    "        return i\n",
    "    can.incan(convs)\n",
    "    can.set_function(call)\n",
    "    return can\n",
    "\n",
    "dc2 = DoubleConv2()\n",
    "out = dc2(input_variable)\n",
    "\n",
    "loss = tf.reduce_mean(out**2.)\n",
    "train_step = opt.minimize(loss, var_list=dc2.get_weights())\n",
    "sess.run(tf.global_variables_initializer()) # init and re-init all the weights (mainly for the optimizer)\n",
    "for i in range(10):\n",
    "    res = sess.run([train_step,loss],feed_dict={})\n",
    "    print('loss:',res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded from test.npy\n",
      "4 weights assigned.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc2.load_weights('test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which is still unintuitive, ugly and verbose\n",
    "\n",
    "Especially if you don't need parameter sharing inside the newly created Can. Well that's the price for all its convenience! Here's another solution if your model is simply a chain of Cans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DoubleConv3():\n",
    "    c = ct.Can()\n",
    "    c.add(ct.Conv2D(3,16,3))\n",
    "    c.add(ct.Conv2D(16,3,3))\n",
    "    c.chain()\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is very close to what you would do with Keras.\n",
    "\n",
    "`c.add()` is equal to `c.incan()`, except that it returns the added Can. `c.chain()` builds the \\_\\_call\\_\\_ function for a Can with all its SubCans, so you don't have to `set_function()` yourself.\n",
    "\n",
    "Everything still works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.62191\n",
      "loss: 2.52024\n",
      "loss: 2.42178\n",
      "loss: 2.3265\n",
      "loss: 2.23437\n",
      "loss: 2.14533\n",
      "loss: 2.05935\n",
      "loss: 1.97639\n",
      "loss: 1.8964\n",
      "loss: 1.81933\n"
     ]
    }
   ],
   "source": [
    "dc3 = DoubleConv3()\n",
    "out = dc3(input_variable)\n",
    "\n",
    "loss = tf.reduce_mean(out**2.)\n",
    "train_step = opt.minimize(loss, var_list=dc3.get_weights())\n",
    "sess.run(tf.global_variables_initializer()) # init and re-init all the variables (mainly for the optimizer)\n",
    "for i in range(10):\n",
    "    res = sess.run([train_step,loss],feed_dict={})\n",
    "    print('loss:',res[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a new Can for your own need\n",
    "\n",
    "please refer to `canton/cans.py`. Here's a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# you know, MLP\n",
    "class Dense(ct.Can):\n",
    "    def __init__(self,num_inputs,num_outputs):\n",
    "        super().__init__()\n",
    "        self.W = self.make_weight([num_inputs,num_outputs])\n",
    "        self.b = self.make_bias([num_outputs])\n",
    "    def __call__(self,i):\n",
    "        W,b = self.W,self.b\n",
    "        d = tf.matmul(i,W)+b\n",
    "        return d"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
